# shellcheck shell=bash
# vim: ft=bash

KVM_IMAGE_NAME="ghcr.io/narkatee/sandbox-kvm-image"
KVM_IMAGE_TAG="latest"
BASE_DISK_ZST="debian-13-nocloud-amd64-custom.qcow2.zst"
BASE_DISK="debian-13-nocloud-amd64-custom.qcow2"
KVM_CACHE="$SANDBOX_CACHE_BASE/kvm-cache"
DISK_IMAGE_ZST="$KVM_CACHE/$BASE_DISK_ZST"
DISK_IMAGE="$KVM_CACHE/$BASE_DISK"
KERNEL_EXTRACT="$KVM_CACHE/vmlinuz"
INITRD_EXTRACT="$KVM_CACHE/initrd"
KVM_BASE_DIR="$SANDBOX_CACHE_BASE/kvm-vms"

kvm_state_dir() {
    local name="$1"
    echo "$KVM_BASE_DIR/$name"
}

kvm_console_socket() {
    local name="$1"
    echo "$(kvm_state_dir "$name")/console.sock"
}

ensure_state_dir() {
    local name="$1"
    local state_dir
    state_dir="$(kvm_state_dir "$name")"
    mkdir -p "$state_dir"
    chmod 700 "$state_dir"
}

is_kvm_running() {
    local name="$1"
    local state_dir
    state_dir="$(kvm_state_dir "$name")"
    local pid_file="$state_dir/qemu.pid"

    if [[ ! -f "$pid_file" ]]; then
        return 1
    fi

    local pid
    pid="$(cat "$pid_file")"

    # Check if process exists and is qemu
    if kill -0 "$pid" 2>/dev/null; then
        if ps -p "$pid" -o comm= | grep -q "qemu"; then
            return 0
        fi
    fi

    # Stale PID file
    rm -f "$pid_file"
    return 1
}

get_kvm_ssh_port() {
    local name="$1"
    local state_dir
    state_dir="$(kvm_state_dir "$name")"
    local port_file="$state_dir/ssh.port"

    if [[ -f "$port_file" ]]; then
        cat "$port_file"
    else
        echo ""
    fi
}

allocate_ssh_port() {
    # Use Python to let kernel allocate ephemeral port
    python3 -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()'
}

fetch_image() {
    # run in a subshell to contain directory change.
    (
        cd "$KVM_CACHE"

        if oras pull "$KVM_IMAGE_NAME:$KVM_IMAGE_TAG" 2>&1; then
            echo "decompressing image..."
            zstd -d "$BASE_DISK_ZST"
        else
            echo "Failed to fetch image." >&2
            exit 1
        fi
    )
}

ensure_kernel() {
    if [ -f "$KERNEL_EXTRACT" ] && [ -f "$INITRD_EXTRACT" ]; then
        return 0
    fi
    echo "Extracting kernel and initrd from image..."
    local img_kernel
    local img_initrd
    img_kernel=$(virt-ls -a "$DISK_IMAGE" /boot 2>/dev/null | grep -i "^vmlinuz" | head -1)
    img_initrd=$(virt-ls -a "$DISK_IMAGE" /boot 2>/dev/null | grep -i "^initrd.img" | head -1)

    if [[ -z "$img_kernel" ]] || [[ -z "$img_initrd" ]]; then
        echo "Error: Could not find kernel/initrd in image" >&2
        exit 1
    fi

    virt-cat -a "$DISK_IMAGE" "/boot/$img_kernel" > "$KERNEL_EXTRACT"
    virt-cat -a "$DISK_IMAGE" "/boot/$img_initrd" > "$INITRD_EXTRACT"
}

ensure_kvm_image() {
    mkdir -p "$KVM_CACHE"

    if ! command -v oras >/dev/null 2>&1; then
        echo "Error: oras not installed (required for KVM image pull)" >&2
        echo "Install: apt-get install oras" >&2
        exit 1
    fi
    if ! command -v virt-cat >/dev/null 2>&1; then
        echo "Error: virt-cat not installed (required for kernel extraction)" >&2
        exit 1
    fi
    if ! command -v virt-ls >/dev/null 2>&1; then
        echo "Error: virt-ls not installed (required for kernel extraction)" >&2
        exit 1
    fi

    if [ -f "$DISK_IMAGE_ZST" ]; then
        echo "Checking image..."
        local sha256sum_local
        local sha256sum_remote
        sha256sum_local="$(sha256sum "$DISK_IMAGE_ZST" | awk '{print $1}')"
        sha256sum_remote="$(oras manifest fetch "$KVM_IMAGE_NAME:$KVM_IMAGE_TAG" | jq -r .layers[0].digest | sed 's/sha256://')"

        if [ "$sha256sum_local" != "$sha256sum_remote" ]; then
            echo "image outdated, updating..."
            rm -f "$KERNEL_EXTRACT" "$INITRD_EXTRACT" "$DISK_IMAGE"
            fetch_image
        fi
    else
        echo "no image, downloading..."
        fetch_image
    fi
    ensure_kernel
}

start_kvm_sandbox() {
    local name="$1"
    local workspace="$PWD"

    ensure_kvm_image
    ensure_state_dir "$name"

    local state_dir
    state_dir="$(kvm_state_dir "$name")"

    local overlay="$state_dir/overlay.qcow2"
    local pid_file="$state_dir/qemu.pid"
    local port_file="$state_dir/ssh.port"
    local console_socket
    console_socket="$(kvm_console_socket "$name")"
    local workspace_file="$state_dir/workspace.path"

    qemu-img create -f qcow2 -b "$(realpath "$DISK_IMAGE")" -F qcow2 "$overlay" 20G >/dev/null
    chmod 600 "$overlay"

    local ssh_port
    ssh_port="$(allocate_ssh_port)"
    echo "$ssh_port" > "$port_file"
    echo "$workspace" > "$workspace_file"

    local ssh_keys
    ssh_keys=$(get_ssh_agent_keys)

    local qemu_args=(
        -enable-kvm
        -machine "pc,smm=off,vmport=off,pit=off,dump-guest-core=off"
        # maximum emulateable feature set, migratable disables host specific features
        -cpu "max,migratable=on"
        -m 8192
        -smp 6
        -nographic
        -monitor none
        -display none
        -nodefaults
        -no-user-config
        -kernel "$KERNEL_EXTRACT"
        -initrd "$INITRD_EXTRACT"
        -append "console=ttyS0 root=/dev/vda3 rw quiet"
        -drive "file=$overlay,if=virtio,cache=writeback,werror=report"
        -chardev "socket,id=serial0,path=$console_socket,server=on,wait=off"
        -device "isa-serial,chardev=serial0"
        # mapped prevents the vm root user to write files as root to the directory
        # Even though that would probably fail because qemu runs a nun root user.
        -virtfs "local,path=$workspace,mount_tag=workspace,security_model=mapped,readonly=off"
        # virtio-rng provides high-quality entropy to guest for crypto operations
        -object "rng-random,id=rng0,filename=/dev/urandom"
        -device "virtio-rng,rng=rng0,max-bytes=1024,period=1000"
        # virtio-balloon enables dynamic memory management
        -device "virtio-balloon"
        # prevent qemu from starting to the background: elevateprivileges=deny,spawn=deny
        -sandbox "on,obsolete=deny,resourcecontrol=deny"
        -rtc "base=utc,clock=vm"
        -daemonize
        -pidfile "$pid_file"
        -fw_cfg "name=opt/com.sandbox/hostname,string=$name"
    )

    # Add SSH keys as fw_cfg if available
    if [[ -n "$ssh_keys" ]]; then
        qemu_args+=(-fw_cfg "name=opt/com.sandbox/ssh_keys,string=$ssh_keys")
    fi

    if [[ $PROXY = true ]]; then
        ensure_kvm_proxy "$name"

        # Get proxy port
        local proxy_port
        proxy_port="$(get_kvm_proxy_port "$name")"
        if [[ -z "$proxy_port" ]]; then
            echo "Error: Could not determine proxy port" >&2
            exit 1
        fi

        local proxy_url="http://10.0.2.100:8888"

        qemu_args+=(
            -fw_cfg "name=opt/com.sandbox/proxy,string=$proxy_url"
            # yes calling netcat for every connection seems a bit strange...
            # for some reason when running with userspace networking the connection handling is
            # strange (at least with the versions on debian trixie)
            # When using guestfwd=tcp:10.0.2.100:8888-tco:127.0.0.1:${proxy_port} (like any sane person)
            # the first connection works totally fine.
            # The second connection however is not working properly. Inside the guest the tcp handshake is successfully completed.
            # So for example curl can complete the handshake with the qemu side of the slip4netns stack.
            # Then: nothing. No package reaches the host systems lo interface (the first request reached the interface).
            # There is something strange happening and I don't care enough to take a deep dive into the
            # network stack ¯\_(ツ)_/¯ netcat it is!
            -netdev "user,id=net0,restrict=on,hostfwd=tcp::${ssh_port}-:2222,guestfwd=tcp:10.0.2.100:8888-cmd:netcat 127.0.0.1 ${proxy_port}"
        )
    else
        qemu_args+=(
            # This network setup gives the guest access to all services running on the host system listening on localhost or *!
            # Why? Because that is the default behaviour of slirp4netns/libslirp, this is what qemu uses to setup userspace networking.
            # There is a --disable-host-loopback option on slirp4netns so libslirp support blocking access to localhost
            # but qemu does not yet offer an option for this:
            # https://gitlab.com/qemu-project/qemu/-/issues/2688
            # Another alternative to try would be "passt" network mode but this is so new in qemu that it's not available on debian.
            -netdev "user,id=net0,hostfwd=tcp::${ssh_port}-:2222"
        )
    fi

    qemu_args+=(
            -device "virtio-net,netdev=net0"
    )

    echo "Starting QEMU..."

    local boot_start
    boot_start=$(date +%s)

    # Run QEMU and try to capture any errors. qemu is not very log-happy when running with -daemonize
    # But we can at-least try...
    set +e
    local qemu_error
    qemu_error=$(qemu-system-x86_64 "${qemu_args[@]}")
    local qemu_exit=$?
    set -e

    if [[ $qemu_exit -ne 0 ]]; then
        echo "Error: QEMU failed to start (exit code: $qemu_exit)" >&2
        if [[ -n "$qemu_error" ]]; then
            echo "QEMU stderr output:" >&2
            echo "$qemu_error" >&2
        fi
        echo "" >&2
        echo "State directory: $state_dir" >&2
        exit 1
    fi

    sleep 1

    if [[ ! -f "$pid_file" ]]; then
        echo "Error: QEMU pid file not created" >&2
        echo "QEMU may have failed after daemonizing" >&2
        rm -rf "$state_dir"
        exit 1
    fi

    local pid
    pid="$(cat "$pid_file")"
    if ! kill -0 "$pid" 2>/dev/null; then
        echo "Error: QEMU process not running (PID: $pid)" >&2
        rm -rf "$state_dir"
        exit 1
    fi

    # Wait for VM to be accessible via SSH
    echo "Waiting for VM to boot..."
    local max_wait=30
    local waited=0
    while ! ssh -o ConnectTimeout=1 \
               -o StrictHostKeyChecking=no \
               -o UserKnownHostsFile=/dev/null \
               -p "$ssh_port" dev@localhost true 2>/dev/null; do
        sleep 1
        waited=$((waited + 1))
        if [[ $waited -ge $max_wait ]]; then
            echo "Error: VM was not reachable via ssh within ${max_wait}s" >&2
            exit 1
        fi
    done

    local boot_end
    boot_end=$(date +%s)
    local boot_time=$((boot_end - boot_start))
    echo "VM booted in ${boot_time}s"

    # Setup SSH alias
    setup_ssh_alias "$name" "$ssh_port"
    if is_ssh_alias_setup "$name"; then
        echo "SSH alias setup: $name"
    else
        echo "SSH port: $ssh_port"
    fi
}

stop_kvm_sandbox() {
    local name="$1"
    local state_dir
    state_dir="$(kvm_state_dir "$name")"
    local pid_file="$state_dir/qemu.pid"

    if [[ -f "$pid_file" ]]; then
        local pid
        pid="$(cat "$pid_file")"
        if kill -0 "$pid" 2>/dev/null; then
            # Send SIGTERM for graceful shutdown
            kill "$pid"

            # Wait for graceful shutdown
            local max_wait=10
            local waited=0
            while kill -0 "$pid" 2>/dev/null && [[ $waited -lt $max_wait ]]; do
                sleep 1
                waited=$((waited + 1))
            done

            # Force kill if still running
            if kill -0 "$pid" 2>/dev/null; then
                kill -9 "$pid" 2>/dev/null || true
            fi
        fi
    fi

    # Cleanup state directory
    if [[ -d "$state_dir" ]]; then
        rm -rf "$state_dir"
    fi

    # Stop KVM proxy
    stop_kvm_proxy "$name"
}

stop_all_kvm_sandboxes() {
    if [[ ! -d "$KVM_BASE_DIR" ]]; then
        return
    fi

    for state_dir in "$KVM_BASE_DIR"/sandbox-*; do
        if [[ ! -d "$state_dir" ]]; then
            continue
        fi

        local name
        name="$(basename "$state_dir")"
        if is_kvm_running "$name"; then
            stop_kvm_sandbox "$name"
        else
            # Clean up stale state
            rm -rf "$state_dir"
        fi
    done

    # Stop all KVM proxy containers
    local proxies
    proxies="$($CONTAINER_ENGINE ps -q --filter "name=^sandbox-proxy-kvm-")"
    if [[ -n "$proxies" ]]; then
        echo "Stopping KVM proxy containers..."
        echo "$proxies" | xargs "$CONTAINER_ENGINE" stop >/dev/null 2>&1 || true
    fi
}

list_kvm_sandboxes() {
    if [[ ! -d "$KVM_BASE_DIR" ]]; then
        echo "No sandboxes running"
        return
    fi

    local found=false
    for state_dir in "$KVM_BASE_DIR"/sandbox-*; do
        if [[ ! -d "$state_dir" ]]; then
            continue
        fi

        found=true
        break
    done

    if ! $found; then
        echo "No sandboxes running"
        return
    fi

    printf "%-30s %-15s %-10s\n" "NAME" "STATUS" "SSH_PORT"

    for state_dir in "$KVM_BASE_DIR"/sandbox-*; do
        if [[ ! -d "$state_dir" ]]; then
            continue
        fi

        local name
        name="$(basename "$state_dir")"
        local port_file="$state_dir/ssh.port"
        local port="N/A"

        if [[ -f "$port_file" ]]; then
            port="$(cat "$port_file")"
        fi

        if is_kvm_running "$name"; then
            printf "%-30s %-15s %-10s\n" "$name" "Running" "$port"
        else
            printf "%-30s %-15s %-10s\n" "$name" "Stopped (stale -> removing...)" "$port"
            # Clean up stale state
            rm -rf "$state_dir"
        fi
    done
}
